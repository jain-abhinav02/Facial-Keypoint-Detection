# -*- coding: utf-8 -*-
"""Facial Keypoints.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oYpovuTv-7b-CV9yUFE2FRzMKxUjN1v8
"""

#before importing the dataset we want to use this code
# The Kaggle API client expects this file to be in ~/.kaggle,
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

# This permissions change avoids a warning on Kaggle tool startup.
!chmod 600 ~/.kaggle/kaggle.json

!kaggle competitions download -c facial-keypoints-detection

!unzip \*.zip

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import cv2
from google.colab.patches import cv2_imshow
from keras.models import Sequential
from keras.layers import Dense,Conv2D,Flatten,MaxPooling2D
from sklearn.model_selection import train_test_split
from keras.applications.vgg16 import VGG16
from keras.applications.vgg16 import preprocess_input
from keras.layers import Convolution2D
from keras.layers import LeakyReLU
from keras.layers import BatchNormalization
from keras.layers import Dropout
from keras.applications.resnet50 import ResNet50
from keras.optimizers import SGD

# Read the training dataset
training = pd.read_csv('training.csv')
training.head()

"""## Old"""

# Handle missing data
for cols in df.columns:
  #print(cols)
  df[cols]=df[cols].fillna(method='ffill');
  df[cols]=df[cols].fillna(method='backfill');

# create images from image column
x_train=np.zeros((df.shape[0],96,96),dtype='float64')
for i in range(df.shape[0]):
  pixels=df['Image'][i].split(' ')
  arr=np.array(pixels)
  arr=arr.reshape((96,96))
  #print(arr.shape)
  arr=np.asarray(arr,dtype='float64')
  x_train[i]=arr

x_train.shape
type(x_train[0])
x_train[0]
x_train
x_train.shape
xx=np.stack([x_train,x_train,x_train],axis=-1)
xx.shape
print(xx[0][0][0])
print(xx[0])
x_train=xx/255.0

df1=df.drop(columns=['Image'])
y_train=df1.to_numpy(dtype='float64')
y_train.shape

print(x_train.shape)
print(y_train.shape)

x_train1,x_test,y_train1,y_test=train_test_split(x_train,y_train,test_size=0.2,random_state=1)

#x_train=np.expand_dims(x_train,axis=3)
#x_train.shape
#x_test=np.expand_dims(x_test,axis=3)
#x_test.shape

def get_model0():
  model=Sequential()
  model.add(Conv2D(32,(3,3),input_shape=(96,96,1),activation='relu'))
  model.add(MaxPooling2D(pool_size=(3,3)))
  model.add(Conv2D(64,(3,3),activation='relu'))
  model.add(MaxPooling2D(pool_size=(3,3)))
  model.add(Flatten())
  model.add(Dense(256,activation='relu'))
  model.add(Dense(96,activation='relu'))
  model.add(Dense(30,activation='relu'))
  return model

def get_model1():
  model=Sequential()
  model.add(VGG16(include_top=False,input_shape=(96,96,3),pooling='max'))
  model.add(Dense(200,activation='relu'))
  model.add(Dropout(0.1))
  model.add(Dense(30))
  return model

def get_model2():
  model=Sequential()
  model.add(ResNet50(include_top=False,input_shape=(96,96,3),pooling='max'))
  model.add(Dense(200,activation='relu'))
  model.add(Dropout(0.1))
  model.add(Dense(30))
  return model

model=get_model2()
model.summary()

opt = SGD(lr=0.01)

model.compile(optimizer='adam',loss='mean_squared_error',metrics=['mae'])

batch_size=256
epochs=10
history=model.fit(x_train1,y_train1,epochs=epochs,batch_size=batch_size,validation_split=0.2)

# Plot training & validation accuracy values
plt.plot(history.history['mean_absolute_error'])
plt.plot(history.history['val_mean_absolute_error'])
plt.title('mean_absolute_error')
plt.ylabel('mae')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

loss,mae=model.evaluate(x_test,y_test)
print(loss)
print(mae)

df2 = pd.read_csv('test.csv')
df2.head()

testing=np.zeros((df2.shape[0],96,96),dtype='float64')
for i in range(df2.shape[0]):
  pixels=df2['Image'][i].split(' ')
  arr=np.array(pixels)
  arr=arr.reshape((96,96))
  #print(arr.shape)
  arr=np.asarray(arr,dtype='float64')
  testing[i]=arr

tt=np.stack([testing,testing,testing],axis=-1)
testing=tt/255.0
testing.shape

res=model.predict(testing)

tab=pd.read_csv('IdLookupTable.csv')
tab.head()

mp= {}
for i,item in enumerate(df.columns):
  mp[item]=i

ans=[]
for i in range(tab.shape[0]):
  imi=tab['ImageId'][i]
  imi=imi-1
  f=tab['FeatureName'][i]
  ind=mp[f]
  ans.append(res[imi][ind])

ans1=np.array(ans,dtype='float64')

ll=[]
for i in range(27124):
  ll.append(i+1)

submission=pd.DataFrame({'RowId':ll,'Location':ans1})
submission.to_csv("submission.csv",index=False)

"""## Visualisation"""

def form_img(s,ind):
  pixels=s.split(' ')
  arr=np.array(pixels)
  arr=arr.reshape((96,96))
  #print(arr.shape)
  arr=np.asarray(arr,dtype='float64')
  #plt.imshow(arr,cmap='gray')
  #cv2.imwrite('trainImages/img'+str(ind)+'.png',arr)
  return arr

for i in range(6):
  form_img(df['Image'][i],i)

left_eye=df[['left_eye_center_x','left_eye_center_y']]
left_eye=left_eye.astype('int64')

right_eye=df[['right_eye_center_x','right_eye_center_y']]
right_eye=right_eye.astype('int64')

def xy(s):
  x=[]
  y=[]
  for i,item in enumerate(s.index):
    if i%2==0:
      x.append(s[item])
    else :
      y.append(s[item])
  return (x,y)

dftmp=df[0:5].drop(columns=['Image'])

res= dftmp.apply(xy,axis=1)

# CV2 : BGR
fig,ax=plt.subplots()
for i in range(1):
  x,y=res[i]
  im=cv2.imread('trainImages/img'+str(i)+'.png')
  ax.imshow(im)
  ax.scatter(x,y,c='r')

"""## New"""

row = training.shape[0]
features = 30

data = training.drop(columns=['Image'])
mask = data.isna()
mask_np = mask.to_numpy()
mask_np = np.sum(mask_np,axis=1)
mask_np = features - mask_np
train_small = data[mask_np <= 8]
train_big = data[mask_np > 8]

train_small.shape

train_big.shape

train_small=train_small.dropna(axis= 'columns', thresh=4000)
train_small.head()

for col in train_small.columns:
  train_small[col]=train_small[col].fillna(train_small[col].median())

null_val = np.zeros(train_big.shape[0])
for col in  train_big:
  null_val+=train_big[col].isna()

null_val.value_counts()

train_big = train_big[null_val<=4]

train_big.shape

for col in train_big.columns:
  train_big[col]=train_big[col].fillna(train_big[col].median())

# create images from image column
x_train_small=np.zeros((train_small.shape[0],96,96),dtype='float64')
for pos,item in enumerate(train_small.index):
  pixels=training['Image'][item].split(' ')
  arr=np.array(pixels)
  arr=arr.reshape((96,96))
  #print(arr.shape)
  arr=np.asarray(arr,dtype='float64')
  x_train_small[pos]=arr

print(train_small.shape)
print(x_train_small.shape)
type(x_train_small[0])

x_rgb_small=np.stack([x_train_small,x_train_small,x_train_small],axis=-1)
x_rgb_small=x_rgb_small.astype('float64')
x_rgb_small=x_rgb_small/255.0

x_rgb_small

# create images from image column
x_train_big=np.zeros((train_big.shape[0],96,96),dtype='float64')
for pos,item in enumerate(train_big.index):
  pixels=training['Image'][item].split(' ')
  arr=np.array(pixels)
  arr=arr.reshape((96,96))
  #print(arr.shape)
  arr=np.asarray(arr,dtype='float64')
  x_train_big[pos]=arr

print(train_big.shape)
print(x_train_big.shape)
type(x_train_big[0])

x_rgb_big=np.stack([x_train_big,x_train_big,x_train_big],axis=-1)
x_rgb_big.shape
x_rgb_big=x_rgb_big/255.0

x_rgb_big.shape

y_train_small=train_small.to_numpy(dtype='float64')
print(y_train_small.shape)
y_train_big=train_big.to_numpy(dtype='float64')
print(y_train_big.shape)

x_train1_small,x_test_small,y_train1_small,y_test_small=train_test_split(x_rgb_small,y_train_small,test_size=0.2,random_state=1)

x_train1_big,x_test_big,y_train1_big,y_test_big=train_test_split(x_rgb_big,y_train_big,test_size=0.2,random_state=1)

print(x_train1_big)

def get_model2_small():
  model=Sequential()
  model.add(ResNet50(include_top=False,input_shape=(96,96,3),pooling='max'))
  model.add(Dense(200,activation='relu'))
  model.add(Dropout(0.1))
  model.add(Dense(8))
  return model

def get_model2_big():
  model=Sequential()
  model.add(ResNet50(include_top=False,input_shape=(96,96,3),pooling='max'))
  model.add(Dense(200,activation='relu'))
  model.add(Dropout(0.1))
  model.add(Dense(30))
  return model

model_small=get_model2_small()
model_big=get_model2_big()

model_small.summary()

model_big.summary()

model_small.compile(optimizer='adam',loss='mean_squared_error',metrics=['mae'])
model_big.compile(optimizer='adam',loss='mean_squared_error',metrics=['mae'])

batch_size=256
epochs=40
history_small=model_small.fit(x_train1_small,y_train1_small,epochs=epochs,batch_size=batch_size,validation_split=0.2)

# Plot training & validation accuracy values
plt.plot(history_small.history['mean_absolute_error'])
plt.plot(history_small.history['val_mean_absolute_error'])
plt.title('mean_absolute_error')
plt.ylabel('mae')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history_small.history['loss'])
plt.plot(history_small.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

loss,mae=model_small.evaluate(x_test_small,y_test_small)
print(loss)
print(mae)

batch_size=256
epochs=40
history_big=model_big.fit(x_train1_big,y_train1_big,epochs=epochs,batch_size=batch_size,validation_split=0.2)

# Plot training & validation accuracy values
plt.plot(history_big.history['mean_absolute_error'])
plt.plot(history_big.history['val_mean_absolute_error'])
plt.title('mean_absolute_error')
plt.ylabel('mae')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history_big.history['loss'])
plt.plot(history_big.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

loss,mae=model_big.evaluate(x_test_big,y_test_big)
print(loss)
print(mae)

"""## Submission time"""

df_test = pd.read_csv('test.csv')
df_test.head()

testing=np.zeros((df_test.shape[0],96,96),dtype='float64')
for i in range(df_test.shape[0]):
  pixels=df_test['Image'][i].split(' ')
  arr=np.array(pixels)
  arr=arr.reshape((96,96))
  #print(arr.shape)
  arr=np.asarray(arr,dtype='float64')
  testing[i]=arr

test_rgb=np.stack([testing,testing,testing],axis=-1)
testing=test_rgb/255.0
testing.shape

tab=pd.read_csv('IdLookupTable.csv')
tab.head()

train_small.shape

train_big.shape

mp1= {}
for i,item in enumerate(train_small.columns):
  mp1[item]=i

mp2= {}
for i,item in enumerate(train_big.columns):
  mp2[item]=i

ans=[]
len=tab.shape[0]
i=0
j=0
while i<len:
  while i<len:
    if tab['ImageId'][i] != tab['ImageId'][j] :
      break
    i+=1
  imi=tab['ImageId'][j]
  imi-=1
  print(i)
  if i-j>8 :
    res=model_big.predict(testing[imi:imi+1])
    while j<i:
      f=tab['FeatureName'][j]
      ind=mp2[f]
      ans.append(res[0][ind])
      j+=1
  else:
    res=model_small.predict(testing[imi:imi+1])
    while j<i:
      f=tab['FeatureName'][j]
      ind=mp1[f]
      ans.append(res[0][ind])
      j+=1

ans1=np.array(ans,dtype='float64')

ll=[]
for i in range(27124):
  ll.append(i+1)

submission=pd.DataFrame({'RowId':ll,'Location':ans1})
submission.to_csv("submission.csv",index=False)

train_small.head()

train_big.head()

